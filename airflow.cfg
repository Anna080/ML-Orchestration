[core]
# The folder where your airflow pipelines live, most likely a
# subfolder in a code repository. This path must be absolute.
dags_folder = /opt/airflow/dags

# The executor class that airflow should use. Choices include
# SequentialExecutor, LocalExecutor, CeleryExecutor,
# KubernetesExecutor, CeleryKubernetesExecutor, LocalKubernetesExecutor
executor = CeleryExecutor

# The SQLAlchemy connection string to the metadata database.
# SQLAlchemy supports many different database engines.
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@postgres/airflow

# The encoding for the databases
sql_engine_encoding = utf-8

# How long before timing out a python file import
dagbag_import_timeout = 60.0

# Secret key to save connection passwords in the db
fernet_key = kyXlqL3L/xGSR7kpPHbWVg==

# Whether to load the DAG examples that ship with Airflow. It's good to
# get started, but you probably want to set this to False in a production
# environment
load_examples = False

# Path to the folder containing Airflow plugins
plugins_folder = /opt/airflow/plugins

# How long before timing out a DagFileProcessor, which processes a dag file
dag_file_processor_timeout = 50

# Whether to disable pickling dags
donot_pickle = True

# Should tasks be executed via forking of the parent process
execute_tasks_new_python_interpreter = False

# Are DAGs paused by default at creation
dags_are_paused_at_creation = True

# The maximum number of active DAG runs per DAG. The scheduler will not create more DAG runs
# if it reaches the limit. This is configurable at the DAG level with max_active_runs.
max_active_runs_per_dag = 16

# The maximum number of task instances that can run concurrently per scheduler
parallelism = 32

# The maximum number of task instances allowed to run concurrently in each DAG.
max_active_tasks_per_dag = 16

# Default timezone in case supplied date times are naive
default_timezone = utc

[webserver]
# The base url of your website: Airflow cannot guess what domain or CNAME you are using.
# This is used to create links in the Log Url column in the Browse - Task Instances menu,
# as well as in any automated emails sent by Airflow that contain links to your webserver.
base_url = http://localhost:8080

# The ip specified when starting the web server
web_server_host = 0.0.0.0

# The port on which to run the web server
web_server_port = 8080

# Secret key used to run your flask app
secret_key = kyXlqL3L/xGSR7kpPHbWVg==

# Number of workers to run the Gunicorn web server
workers = 4

[logging]
# The folder where airflow should store its log files.
# This path must be absolute.
base_log_folder = /opt/airflow/logs

# Logging level.
logging_level = INFO

# Flag to enable/disable Colored logs in Console
colored_console_log = True

# Log format for when Colored logs is enabled
#colored_log_format = [%(blue)s%(asctime)s%(reset)s] {%(blue)s%(filename)s:%(reset)s%(lineno)d} %(log_color)s%(levelname)s%(reset)s - %(log_color)s%(message)s%(reset)s

# Format of Log line
#log_format = [%(asctime)s] {%(filename)s:%(lineno)d} %(levelname)s - %(message)s

[scheduler]
# Task instances listen for external kill signal (when you clear tasks
# from the CLI or the UI), this defines the frequency at which they should
# listen (in seconds).
job_heartbeat_sec = 5

# The scheduler constantly tries to trigger new tasks (look at the
# scheduler section in the docs for more information). This defines
# how often the scheduler should run (in seconds).
scheduler_heartbeat_sec = 5

# The frequency (in seconds) at which the LocalTaskJob should send heartbeat signals to the
# scheduler to notify it's still alive. If this value is set to 0, the heartbeat interval will default
# to the value of scheduler_zombie_task_threshold.
local_task_job_heartbeat_sec = 0

# Number of seconds after which a DAG file is parsed. The DAG file is parsed every
# min_file_process_interval number of seconds. Updates to DAGs are reflected after
# this interval. Keeping this number low will increase CPU usage.
min_file_process_interval = 30

# How often (in seconds) to check for stale DAGs (DAGs which are no longer present in
# the expected files) which should be deactivated, as well as datasets that are no longer
# referenced and should be marked as orphaned.
parsing_cleanup_interval = 60

# How long (in seconds) to wait after we have re-parsed a DAG file before deactivating stale
# DAGs (DAGs which are no longer present in the expected files).
stale_dag_threshold = 50

# How often (in seconds) to scan the DAGs directory for new files. Default to 5 minutes.
dag_dir_list_interval = 300

# How often (in seconds) should pool usage stats be sent to StatsD (if statsd_on is enabled)
pool_metrics_interval = 5.0

# If the last scheduler heartbeat happened more than scheduler_health_check_threshold
# ago (in seconds), scheduler is considered unhealthy.
scheduler_health_check_threshold = 30

# How often (in seconds) should the scheduler check for orphaned tasks and SchedulerJobs
orphaned_tasks_check_interval = 300.0

# Determines the directory where logs for the child processes of the scheduler will be stored
child_process_log_directory = /opt/airflow/logs/scheduler

# Local task jobs periodically heartbeat to the DB. If the job has
# not heartbeat in this many seconds, the scheduler will mark the
# associated task instance as failed and will re-schedule the task.
scheduler_zombie_task_threshold = 300

# How often (in seconds) should the scheduler check for zombie tasks.
zombie_detection_interval = 10.0

# Max number of DAGs to create DagRuns for per scheduler loop.
max_dagruns_to_create_per_loop = 10

# How many DagRuns should a scheduler examine (and lock) when scheduling
# and queuing tasks.
max_dagruns_per_loop_to_schedule = 20

# Should the Task supervisor process perform a "mini scheduler" to attempt to schedule more tasks of the
# same DAG. Leaving this on will mean tasks in the same DAG execute quicker, but might starve out other
# dags in some circumstances
schedule_after_task_execution = True

# The scheduler can run multiple processes in parallel to parse dags.
# This defines how many processes will run.
parsing_processes = 2

# One of modified_time, random_seeded_by_host and alphabetical.
# The scheduler will list and sort the dag files to decide the parsing order.
file_parsing_sort_mode = modified_time

[celery]
# The app name that will be used by celery
celery_app_name = airflow.providers.celery.executors.celery_executor

# The concurrency that will be used when starting workers with the
# airflow celery worker command.
worker_concurrency = 16

# Used to increase the number of tasks that a worker prefetches which can improve performance.
worker_prefetch_multiplier = 1

# Specify if remote control of the workers is enabled.
worker_enable_remote_control = true

# The Celery broker URL. Celery supports RabbitMQ, Redis and experimentally
# a sqlalchemy database.
broker_url = redis://redis:6379/0

# The Celery result_backend. When a job finishes, it needs to update the
# metadata of the job. Therefore it will post a message on a message bus,
# or insert it into a database (depending of the backend)
result_backend = db+postgresql://airflow:airflow@postgres/airflow

# How many processes CeleryExecutor uses to sync task state.
sync_parallelism = 0

task_acks_late = True

# Celery task will report its status as 'started' when the task is executed by a worker.
task_track_started = True

# The Maximum number of retries for publishing task messages to the broker when failing
# due to AirflowTaskTimeout error before giving up and marking Task as failed.
task_publish_max_retries = 3

# Worker initialisation check to validate Metadata Database connection
worker_precheck = False

task_reject_on_worker_lost = True

worker_max_tasks_per_child = 1

worker_task_soft_time_limit = 3600

worker_task_time_limit = 3660

[operators]
# The default owner assigned to each new operator, unless
# provided explicitly or passed via default_args
default_owner = airflow

# Default queue that tasks get assigned to and that worker listen on.
default_queue = default

[email]
# Configuration email backend and whether to
# send email alerts on retry or failure
email_backend = airflow.utils.email.send_email_smtp

[smtp]
# If you want airflow to send emails on retries, failure, and you want to use
# the airflow.utils.email.send_email_smtp function, you have to configure an
# smtp server here
smtp_host = localhost
smtp_starttls = True
smtp_ssl = False
smtp_port = 25
smtp_mail_from = airflow@example.com
